{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "firstNeuralNetwtork",
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyOw5WqEfPK0Ho6Gu/fevReG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohans1029/pythonPlayground/blob/master/firstNeuralNetworkMNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yT0QiMq2euGN",
        "colab_type": "text"
      },
      "source": [
        "This is my attempt at a neural network! We will use the MNIST Dataset-the \"Hello, world!\" machine learning problem. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKsAANucfMvJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCyGL1FlfnNy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKaLLeMUgPL1",
        "colab_type": "text"
      },
      "source": [
        "We have just imported tensorflow and keras, the two things needed for this network. Keras already comes with common datasets in it, so all we needed to do is import the *mnist* dataset from *keras.datasets*. Then, we assign the data to mnist.load_data() with the classes (labels) and training/testing noted. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYcYmv4rhDBw",
        "colab_type": "text"
      },
      "source": [
        "As we know, computers read images as a bunch of different numbers. To break it down, we must define the word \"tensor\" as another word for dimension. When a figure is three-dimensional, we mean the figure is a 3d tensor. The following heirarchy best describes this: \n",
        "\n",
        "\n",
        "*   0D tensor: a single number (scalar) \n",
        "*   1d tensor: an array (vector) \n",
        "*   2d tensor: a 2d array (matrix) \n",
        "*   3d tensor: 2d tensors in another array \n",
        "*   nd tensor: n-1d tensor in another array\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8LR_t37iAR7",
        "colab_type": "text"
      },
      "source": [
        "Now that we have tensors covered, we must define the meaning of \"shape\". \n",
        "\n",
        "A shape is a tuple (sequence in python)  of integers that describes how many dimensions the tensor has along each axis (the length of the sequence equals the dimensions of the tensor-a length of 4 would mean the data is 4d). \n",
        "\n",
        "Image datasets are generally read as 3d or 4d tensors (3d if grayscale and 4d if rbg, because their is another value in the shape array for the color).  \n",
        "\n",
        "Take for example, the mnist training set shape: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUSNA1P6jngE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_images.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzZgfFHjjz0k",
        "colab_type": "text"
      },
      "source": [
        "\">>>\" (60000, 28, 28), since the dataset is composed of grayscale images. the shape tuple is of length 3, meaning the shape is three dimensions. The first number in the tuple is 60000 meaning that their are 60000 images in our training test set. The second number is 28 and represents the height of the image. The third number is 28 and represents the width of the image. In summary: (# of samples, height, width). \n",
        "\n",
        "Now if the image was in color, the shape would be 4d and the shape tuple would be like this: (# of samples, height, width, channels) where channels mean the amount of color values for each image-usually 3 if using the rbg scale (one number for each color value). \n",
        "\n",
        "It also worth noting that a grayscale image can also be a 4d tuple by simply making the fourth integer a 1, representing a number for the grayscale value (60000, 28, 28, 1). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m04vBDCnmbea",
        "colab_type": "text"
      },
      "source": [
        "Now lets see the data labels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-L6v7t2msAM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BScTd7j4RqxI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6gHD4b5nV3C",
        "colab_type": "text"
      },
      "source": [
        "Now it is time to build the neural network. To describe the workflow, we will feed the network the train dataset for it to learn the association between the images and labels. Then we'll test it by asking the network to produce predictions for test_images. Then we will verify to see how well the predictions match the labels.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJ1r5GQ-n8WA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import models and layers for keras. \n",
        "from keras import models \n",
        "from keras import layers \n",
        "\n",
        "# import the Sequential model from models \n",
        "network = models.Sequential() \n",
        "# create the layers \n",
        "# Dense layer for image filtering to produce representations\n",
        "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
        "# 10-way softmax layer that returns an array of 10 probability scores (summing to 1)\n",
        "network.add(layers.Dense(10, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IovOuUcnpWai",
        "colab_type": "text"
      },
      "source": [
        "We have just built a network (a very simple one) consisting of a Sequential model (linear, input/output) defined as *network* and two layers. One layer is a dense layer that is the first input layer in which feature representations are learned. Then those representations are sent into the next layer known as the classifier layer (also a dense layer). This layer takes in the feature representations it learned in the previous layer and spits out an array of 10 numbers, each of which are a double displaying the probability that the image is that respective number (like percents in which the complete sum of all the numbers will be one). This layer does this through the activation function *softmax*. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jboEtwGqt3j",
        "colab_type": "text"
      },
      "source": [
        "To better understand what is under the hood here, take François Chollet's explanation: \n",
        "\"The core building block of neural networks is the layer, a data-processing module that you can think of as a filter for data. Some data goes in, and it comes out in a more useful form. Specifically, layers extract representations out of the data fed into them—hopefully, representations that are more meaningful for the problem at hand. Most of deep learning consists of chaining together simple layers that will implement a form of progressive data distillation. A deep-learning model is like a sieve for data processing, made of a succession of increasingly refined data filters—the layers. \n",
        "\n",
        "Here, our network consists of a sequence of two Dense layers, which are densely connected (also called fully connected) neural layers. The second (and last) layer is a 10-way softmax layer, which means it will return an array of 10 probability scores (summing to 1). Each score will be the probability that the current digit image belongs to one of our 10 digit classes.\"-Deep Learning with Python. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQKWw1cordEc",
        "colab_type": "text"
      },
      "source": [
        "Now since the network is done being built, we are almost ready for training. First, however, we need to do what's called the \"compilation step\" consisting of defining three things: \n",
        "\n",
        "\n",
        "*   **Loss function**: a fuction used during training that helps the network measure its performance on the training data, thus helping steer itself in the right direction (helps with making adjustments (optimizer does this, and the loss function supplies the information) to the representations learned in the network). \n",
        "*   **Optimizer**: Françis Chollet says it best \"optimizer—The mechanism through which the network will update itself based on the data it sees and its loss function.\"-Deep Learning with Python. \n",
        "*   **Monitor metrics during training and testing**: accuracy (for this case). \n",
        "\n",
        "In summary, the loss function supplies the information needed for the network to utilize the optimizer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXW8QtwItk5t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "network.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CC3z7F4atyMT",
        "colab_type": "text"
      },
      "source": [
        "Now we enter the \"preprocessing step\" in which we reshape the data into the shape the network expects. We also need to scale it so that all values are in the \"[0,1]\" interval. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzT9cz5ruWCt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reshape into a 2d tuple array. \n",
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "# transform in float32 array with values in between 0 and 1. \n",
        "train_images = train_images.astype('float32') / 255\n",
        "\n",
        "# do the same for testing images.\n",
        "test_images = test_images.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype('float32') / 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_F_0GVrSu2fU",
        "colab_type": "text"
      },
      "source": [
        "Now that we are done with the two main steps after building the network (compiling, and preprocessing) we have one final step: preparing the labels. We categorically encode the labels as follows:   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aL6DWTQva57",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import to_categorical from keras utility. \n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# assign to both training and testing labels. \n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHmxSvYzvunb",
        "colab_type": "text"
      },
      "source": [
        "We are now done with the previous steps and are ready for training! To do this, we use the *fit method* in keras, essentially \"*fit*ting the model to its training data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UshtPmVwGjX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Assigning the fit method to training data. \n",
        "history = network.fit(train_images, train_labels, epochs=5, batch_size=128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgZtmvRjBNg_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(history.history.keys())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eSPek6IwaD2",
        "colab_type": "text"
      },
      "source": [
        "Done! that is it for training. \n",
        "In a later thread, I will elaborate in what all of this *training* and things related to training. It is definately important to understand what is going on under the hood. For now, I will summarize the *training (fit)* line of code. \n",
        "network.fit(train_images, train_labels, epochs=5, batch_size=128) \n",
        "in the dataset we defined, put in the **images** and the respective **labels**: (train_images, train_labels,. Now define how many epochs you want to cover. **Epochs** means one full iteration through the data. So five epochs here, means the network iterates over the complete training data five times. **The batch size** refers to how many images the model processing at once (\"Deep-learning models don’t process an entire dataset at once; rather, they break the data into small batches.\"-Deep learning with Python, Françis Chollet). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8qlgrcu27td",
        "colab_type": "text"
      },
      "source": [
        "It also important to note that during training, the two values, loss and accuracy, are stated for each epoch. The 5/5 epoch accuracy for this network reached an accuracy of 0.9875 or 98.7% "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqGNrYrB3qZ9",
        "colab_type": "text"
      },
      "source": [
        "Although we already see the accuracy numbers during training, we now must perform the ***test*** step. This is because we need to see how the network performs on data that it has never seen before. This gives rise to the concept of **overfitting** which we will define later, in another thread, in depth. Briefly, overfitting occurs when the network performs significantly worse on the test data then on the training data. Thus, to see if the network overfits, we perform the *test* step. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhJtZcR85B9q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loss, test_acc = network.evaluate(test_images, test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pScz4Ynv5XqE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Test accuracy: ', test_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvfZt6EyD2bp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Test loss: ', test_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duuCysmQ5h6O",
        "colab_type": "text"
      },
      "source": [
        "Now we are done! The model does somewhat overfit as the test accuracy is 98% which is slightly lower than 99.7%. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BObI89a-5z9F",
        "colab_type": "text"
      },
      "source": [
        "We can stop here, but for the purpose of further understanding and research, we can use some of pythons other libraries to help us visualize the results. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ghgu7xfK6FWd",
        "colab_type": "text"
      },
      "source": [
        "Lets start with visualizing the training accuracy through **matplotlib** using **numpy**: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBM6EGWC_dhf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy \n",
        "plt.plot(history.history['acc'])\n",
        "plt.title('Training Accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvFD7QZDCmEa",
        "colab_type": "text"
      },
      "source": [
        "Next we can visualize the training loss: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3tKw6Z4Co9d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.title('Training Loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TA3-vcl2D-gQ",
        "colab_type": "text"
      },
      "source": [
        "At the moment, we cannot visualize the test loss and accuracy. However, you can usually visualize \"*validation*\", but for this model, we didn't divide the data into validation. "
      ]
    }
  ]
}